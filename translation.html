<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<!-- Latest compiled and minified CSS -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

		<!-- Optional theme -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">

		<!-- Latest compiled and minified JavaScript -->
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

		<link rel="stylesheet" href="style.css" />

		<title>Translating research into practice</title>

	</head>
	<body>

		<p><a href="index.html">Back to table of contents</a></p>

		<img src="images/TBD.jpg" class="img-responsive" alt="TBD" />
		<small>TBD</small>

		<h1>Translating research into practice</h1>
		<div class="lead">Andrew J. Ko</div>
    
		<p>
			I love doing research on interfaces.
			There's nothing like imagining an entirely new way of interacting with a computer, creating it, and then showing it to the world.
			But if the inventions researchers like myself create never impact the interfaces we all use every day, what's the point?
			The answer, of course, is more complex than just a binary notion of an invention's impact.
		</p>

		<p>
			First, much of what you've read about in this book has <em>already</em> impacted other researchers.
			This type of research impact is critical: it shapes what other inventors think is possible, provides them with new ideas to pursue, and can sometimes catalyze entirely new genres.
			Think back, for example, to Vannevar Bush's Memex.
			No one actually made a Memex as described, nor did they need to; instead, other inventors selected some of their favorite ideas from his vision, combined them with other ideas, and manifested them in entirely unexpected ways.
			The result was more than just more research ideas, but eventually products and entire networks of computers that have begun to reshape society.
		</p>

		<p>
			How then, do research ideas <em>indirectly</em> lead to impact?
			Based on both the experience of HCI researchers and practitioners attempting to translate research into practice (<a href="#colluso">Colusso et al. 2019</>) and historical records of interface innovation, there are three essential milestones that must occur (at least in a capitalist economy):
		</p>

		<ul>
			<li>
				<strong>Researchers must demonstrate feasibility</strong>.
				Until we know that something works, that it works well, and that it works consistently, it just doesn't matter how exciting the idea is.
				It took Xerox PARC years to make the GUI feel like something that could be a viable product.
			</li>
			<li>
				<strong>Someone must take an entreprenurial risk</strong>.
				It doesn't matter how much evidence researchers gather about the viability of an interface idea.
				At some point, someone in an organization is going to have to see some opportunity in bringing that interface to the world at scale.
				Even if the interface is fully proven (which it never is), scale is its own risk.
				These organizations might be startups, established companies, government entities, non-profits, open source communities, or any other form of community that wants to make something.
			</li>
			<li>
				<strong>The interface must win in the marketplace</strong>.
				Even if an organization perfectly executes an interface at scale, and is fully committed to seeing it in the world, it only lives on if the timing is right for people to want it relative to other interfaces.
				Consider Google's many poorly timed innnovations at scale: Google Glass, Google Wave, Google Buzz, Google Video; all of these were compelling new interface innovations, but none of them found a market, and so they didn't survive.
		</ul>

		<p>
			Let's consider four examples of interface technologies where these three things either did or didn't happen, and that determined whether the idea made it to market.
		</p>

		<img src="images/rosie.jpg" class="img-responsive" alt="Rosie the robot maid, from The Jetsons, cleaning up sandwich debris." />

		<p>
			The first example we'll consider is what's typically known as <strong>strong AI</strong>.
			This is the idea of a machine that exhibits behavior that is at least, if not more skillful than human behavior.
			This is the kind of AI portrayed in many science fiction movies, usually where the AI either takes over humanity (e.g., <a href="https://en.wikipedia.org/wiki/The_Terminator">The Terminator</a>), or plays a significant role in human society (e.g., <a href="https://en.wikipedia.org/wiki/List_of_The_Jetsons_characters#Rosie">Rosie the robot maid in the Jetsons</a>).
			These kinds of robots, in a sense, are an interface: we portray interacting with them, giving them commands, and utilizing their output.
			The problem of course, is that strong AI isn't (yet) feasible.
			No researchers have demonstrated any form of strong AI.
			All AI to date has been <strong>weak AI</strong>, capable of functioning in only narrow ways after significant human effort to gather data to train the AI.
			Because strong AI is not feasible in the lab, there aren't likely to be any entreprenurs willing to take the risk of bringing something to market at scale.
		</p>
		
		<img src="images/bci.jpg" class="img-responsive" alt="Two people with sensors on their heads attempting to operate a brain computer interface." />

		<p>
			That example was fairly obvious of course; something that doesn't work clearly isn't going to be transferred into practice.
			But what about something that <em>is</em> feasible based on research?
			Consider, for example, brain-computer interfaces, which have a reasonable body of evidence behind them.
			We know that, for example, it's feasible to detect muscular activity with non-invasive sensors and that we can classify a large range of behaviors based on this.
			The key point that many researchers fail to recognize is that such evidence of feasibility is <em>necessary but insufficient</em> to motivate a business risk.
			To bring brain-computer interfaces to market, one needs a plan for who will pay for that technology and why.
			Will it be a popular game or gaming platform that causes people to buy?
			A context where hands-free, voice-free input is essential and valuable?
			Or perhaps someone will bet on creating a <em>platform</em> on which millions of application designers might experiment, searching for that killer app?
			Whatever happens, it will be a market opportunity that pulls research innovations from the archives of digital libraries and researcher's heads into the product plans of an organization.
		</p>

		<p>
			Of course, just because someone sees an opportunity doesn't mean that there actually is one, or that it will still exist by the time a product is released.
			Consider, for example, Google Glass, which was based on decades of augmented reality HCI research, led by Georgia Tech researcher <a href="https://www.wired.com/story/google-glass-predicted-the-future/">Thad Starner</a>.
			Starner eventually joined Google, where he was put in charge of designing and deploying Googlge Glass.
			The vision was real, the product was real, and some people bought them (for $1,500).
			The release was more of a beta in terms of functionality.
			People weren't ready to constantly say "OK, Glass" every time they wanted it to do something.
			And the public was <em>definitely</em> not ready for people wandering around with a recording device on their face.
			The knickname "Glasshole" was coined, and suddenly, the cost of wearing the device wasn't just financial, but social.
			Google left the market in 2015, largely because there <em>wasn't</em> a market to sell to (yet, or possibly ever).
		</p>
			
		<p>
			Voice assistants: research feasibility, Apple took the risk, found a market.
		</p>
		
		<p>
			What are the implications of these stories for someone in innovating in industry?
			The criteria is pretty clear, even if the strategies for success aren't.
			First, if an interface idea hasn't been rigorously tested in research, building a product (or even a company) out of the idea is very risky.
			That puts a product team or company in the position of essentially <em>doing</em> research, and as we know, research doesn't always work out.
			Some companies (e.g., Waymo), decide to take on these high risks, but they often do so with the high expectation of failure.
			Few companies can do that.
		</p>

		<p>
			Even when an idea is great and we know it works, there's a really critical phase in which someone has to learn about the idea, see an opportunity, and take a leap to invest in it.
			Who's job is it to ensure that entrepreners and companies learn about research ideas?
			Should researchers be obligated to market their ideas to companies?
			If so, how should researchers get their attention?
			Should product designers be obligated to visit academic conferences, read academic journals, or read books like this?
			Why should they, when the return on investment is so low?
			In other disciplines such as medicine, there are people who practice what's called <em>translational medicine</em>, in which researchers take basic medical discoveries and try to find product opportunities for them.
			These roles are often funded by governemnts, which view their role as investing in things markets cannot risk doing.
			Perhaps computing should have the same roles and government investment.
		</p>
		
		<p>
			Finally, and perhaps most importantly, even when there <em>are</em> real opportunities for improving products through new interface ideas, the timing in the world has to be right.
			People may view the benefit of learning a new interface as too low relative to the cost of learning.
			There may be other products that have better marketing, or that have found ways of locking customers in.
			These market factors have nothing to do with the merits of an idea, but the particular structure of a marketplace at a particular time.
		</p>
		
		<p>
			The result of these three factors is that the gap between research and practice is quite wide.
			We shouldn't be surprised that innovations from academia can take decades to make it to market, if ever.
			If you're reading this book, consider your role in mining research for innovations and bringing them to products.
			Are you in a position to take a risk on bringing a research innovation to the world?
			If not, who is?
		</p>
		
		<center><p class="lead"><a href="index.html">Back to table of contents</a></p></center>

		<h2>Further reading</h2>

		<p id="colusso">Lucas Colusso, Ridley Jones, Sean A. Munson, and Gary Hsieh (2019). A Translational Science Model for HCI. ACM SIGCHI Conference on Human Factors in Computing, to appear.</p>
			
		<script type="text/javascript">
		
			var _gaq = _gaq || [];
			_gaq.push(['_setAccount', 'UA-10917999-1']);
			_gaq.push(['_trackPageview']);
			
			(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
			})();
		
		</script>

	</body>

</html>
